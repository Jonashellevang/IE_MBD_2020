# Data Iintegration Design (Pentaho)
During the first assignment we created a data warehouse (or better said, a data mart) based on a specific data set. This second assignment was focused on Data Integration with Pentaho. It consisted on designing an one-time historic data load using an ETL (Extraction, Transformation, Loading) tool. That means create an ETL process for each dimension and fact table(s) from the [previous assignment](https://github.com/Jonashellevang/IE_MBD_2020/tree/master/Data%20Warehouse%20Modelling%20(MySQL))

## Deliverables
* Data Mapping document: this document describes the default strategy for extracting data from data source, data mapping process and data quality tracking and metadata approach.
* ETL scripts: The collection of ETL processes created with Pentaho Data Integration. 
* Database backup: after the ETL process has been executed with success.
