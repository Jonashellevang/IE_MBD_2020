# Data Integration Design (Pentaho)
During the first assignment we created a data warehouse (or better said, a data mart) based on a specific data set. This second assignment was focused on Data Integration with Pentaho. It consisted on designing a one-time historic data load using an ETL (Extraction, Transformation, Loading) tool. That means create an ETL process for each dimension and fact table(s) from the [previous assignment](https://github.com/Jonashellevang/IE_MBD_2020/tree/master/Data%20Warehouse%20Modelling%20(MySQL)).

## Deliverables
* [Data Mapping document](https://github.com/Jonashellevang/IE_MBD_2020/blob/master/Data%20Integration%20Design%20(Pentaho)/Data%20Mapping%20Instructions%20Report.pdf): this document describes the default strategy for extracting data from data source, data mapping process and data quality tracking and metadata approach.
* [ETL scripts](https://github.com/Jonashellevang/IE_MBD_2020/tree/master/Data%20Integration%20Design%20(Pentaho)/Data_integration_groupF): The collection of ETL processes created with Pentaho Data Integration. This is a folder, so there is many files stored here.
