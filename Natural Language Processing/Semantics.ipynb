{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet\n",
    "\n",
    "NLTK provides a useful WordNet interface to play with the WordNet data (included into the `nltk.corpus`). Let's see how to use it\n",
    "\n",
    "First we import the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `wn` object to get the synsets of a given word.\n",
    "\n",
    "For instance, thise are the synsets related to the word `dog`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words may have different POS tag synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('fish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter by POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('fish', pos=wn.VERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much more than expected. In order to make sense of each sense, we can plot their definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in wn.synsets('fish'):\n",
    "    print(synset)\n",
    "    print(synset.definition())\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or examples for each synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in wn.synsets('dog'):\n",
    "    print(synset)\n",
    "    print(synset.examples())\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or lemmas related to the synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in wn.synsets('dog'):\n",
    "    print(synset)\n",
    "    print(synset.lemma_names())\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cool feature of the NLTK WordNet corpus is that it gives access to the **Open Multilingual WordNet**.\n",
    "\n",
    "It is useful to, for instance, get the lemmas in another languages a given synset, through the function `lemma_names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('omw')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in wn.synsets('dog'):\n",
    "    print(synset)\n",
    "    print(synset.lemma_names(lang=\"ita\"))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on the first dog sysnet.\n",
    "\n",
    "We can access to its relationships (`hypernyms`, `hyponyms`, `holonyms`, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "print(dog.hypernyms())\n",
    "print(\"---------------\")\n",
    "print(dog.hyponyms())\n",
    "print(\"---------------\")\n",
    "print(dog.member_holonyms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some relations are defined by WordNet only over Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = wn.synset('good.a.01')\n",
    "good.lemmas()[0].antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK also has implemented the **path-based similarity** function that we explained in class by means of the function `path_similarity`. It returns a score denoting how similar two word senses are, based on the shortest path that connects the senses in the is-a (hypernym/hypnoym) taxonomy. A score of 1 represents identity i.e. comparing a sense with itself will return 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "hit = wn.synset('hit.v.01')\n",
    "slap = wn.synset('slap.v.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog.path_similarity(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit.path_similarity(slap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has also the [IC-based](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.59.2199) similarity. To that end you have to load  an information content file from the `wordnet_ic` corpus and then use this information with the `res_similarity` function to compute the IC-based similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "dog.res_similarity(cat, brown_ic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer it, you can also train you own IC dictionary from any corpus. This is very useful if you want to compute the similary between words based on some particular data that you have for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import genesis\n",
    "genesis_ic = wn.ic(genesis, False, 0.0)\n",
    "dog.res_similarity(cat, genesis_ic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMI\n",
    "\n",
    "In addition to the thesaurus-based metrics, we can also create similarity functions based on Distributional algorithms; that is, words that appear in similar contexts are expected to be similar.\n",
    "\n",
    "In particular, in class we presented the Point-wise Mutual Information as a measure the set the similarity of two words based on their contexts. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find words that appear in the same context is actually quite easy by using NLTK's `Text.similar()` function. This function takes a word w, finds all contexts w1w w2, then finds all words w' that appear in the same context, i.e. w1w' w2. (You can find the implementation online at http://nltk.org/nltk/text.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "\n",
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
    "text.similar('man')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `PMI` to compute similarities between words.\n",
    "\n",
    "To that end, I have defined a function that takes two words, a dictionary with the frequency of the words `unigram_freq` and another dictionary `bigram_freq` with the count of each pair of words in the corpus. \n",
    "\n",
    "With these two dicts we can compute the joint probability of each pair of words (calculated as the fraction of the number of times they appear together and the total frequency of pairs of words) and, finally, compute the PMI as the fraction of the joint probability and the product of the marginal probabilites of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(word1, word2, unigram_freq, bigram_freq):\n",
    "    import math\n",
    "    marginal_word1 = float(unigram_freq[word1]) / sum(unigram_freq.values())\n",
    "    marginal_word2 = float(unigram_freq[word2]) / sum(unigram_freq.values())\n",
    "    joint_w1_w2 = float(bigram_freq[(word1, word2)])/sum(bigram_freq.values())\n",
    "    pmi = round(math.log(max(0.0005,joint_w1_w2/(marginal_word1*marginal_word2)),2),2)\n",
    "    return pmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has a package `collocations` that makes quite easy to compute the count of each pair of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.collocations.BigramCollocationFinder.from_words(nltk.corpus.brown.words(categories='news'), window_size = 20)\n",
    "bigrams.apply_freq_filter(20)\n",
    "bigrams_freq = bigrams.ngram_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `FreqDist` function (which we already knew) to compute the individual frequencies of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = nltk.FreqDist( nltk.corpus.brown.words(categories=\"news\"))\n",
    "unigrams_freq = {unigram:freq for unigram, freq in unigrams.items() if freq >= 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the defined `pmi` function to compute the PMI similarity of two words.\n",
    "Let us see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi(u\"day\", u\"night\", unigrams_freq, bigrams_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi(u\"per\", u\"cent\", unigrams_freq, bigrams_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi(u\"day\", u\"administration\", unigrams_freq, bigrams_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi(u\"government\", u\"administration\", unigrams_freq, bigrams_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK also provides some useful classes inside the `collocations` package to automatically compute this PMI-based similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations are expressions of multiple words which commonly co-occur. For example, the top ten bigram collocations in Brown news corpus are listed below, as measured using Pointwise Mutual Information (by using the `bigram_measures.pmi` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(nltk.corpus.brown.words(categories='news'), window_size = 20)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While these words are highly collocated, the expressions are also very infrequent. Therefore it is useful to apply filters, such as ignoring all bigrams which occur less than 20 times in the corpus and removing the stopwords:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder.apply_freq_filter(20)\n",
    "ignored_words = nltk.corpus.stopwords.words('english')\n",
    "finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "finder.nbest(bigram_measures.pmi, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec\n",
    "\n",
    "Using Wor2vec in Python is in fact quite straightforward thanks to the package `gensim` (https://radimrehurek.com/gensim/), which has a package focused on Word2vec where you can create your own embeddings from a dataset.\n",
    "\n",
    "For more information on the generation of embeddings with this package, you can follow this tutorial: http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.WszQTXVuZhE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word2vec\n",
    "\n",
    "The following code creates two embeddings model, one for the brown corpus and one for the movie_reviews dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown, movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Word2Vec(brown.sents(), hs=1, negative=0, size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = Word2Vec(movie_reviews.sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once trained the models, we can compute similarities between words. Try different words and check the differences between the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.wv.most_similar('man', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.wv.most_similar('man', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.wv.most_similar('movie', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.wv.most_similar('movie', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mr.wv.similarity('man', 'woman'))\n",
    "print(b.wv.similarity('man', 'woman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mr.wv.similarity('man', 'car'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.wv.doesnt_match(\"automobile car dinner\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b.wv.most_similar(positive=['father','doctor'], negative=['mother']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, you need to download the pretrained model from [https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit). This model has been trained with billions of words, for a vocabulary of 3 million words. The file itself is around 1.6GB and it is provided as a \".gz\" compressed file (you need to decompressed first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.keyedvectors as word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model. This may take a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to downloaded file (unzipped)\n",
    "#adapt to your system, the following would my path\n",
    "#path = \"C:\\Users\\Idafen Santana Perez\\Downloads\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "path = \"YOUR_SYSTEM_PATH\\GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "#loading the downloaded model\n",
    "model = word2vec.KeyedVectors.load_word2vec_format(path, binary=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the vector of the word 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = model['cat']\n",
    "print(cat[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try the king-man+woman operation. Both 'king' and 'woman' are positive, while 'man' is a negative value.\n",
    "\n",
    "This analogy can be read as \"Man is to King, what Woman is to ______\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try now doctor-father+mother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.most_similar(positive=['mother', 'doctor'], negative=['father'], topn=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now a bit of geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.most_similar(positive=['Spain', 'Paris'], negative=['France'], topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.most_similar(positive=['Madrid', 'Tenerife'], negative=['Gran_Canaria'], topn=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test similarity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.similarity('woman', 'man'))\n",
    "print(model.similarity('car', 'man'))\n",
    "print(model.similarity('fridge', 'man'))\n",
    "print(model.similarity('fridge', 'woman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check how typos relate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.most_similar(positive=['because','teh'],negative=['the']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how other words, such as groups of animals or name are somehow analogous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.most_similar(positive=['fish','flock'],negative=['birds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.most_similar(positive=['Jennifer','he'],negative=['she']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (p3env)",
   "language": "python",
   "name": "p3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
