# Class Exercises
### [Language Models.ipynb](https://github.com/Jonashellevang/IE_MBD_2020/blob/master/Natural%20Language%20Processing/Class%20Exercises/Language%20Models.ipynb)
In this notebook we went through creating sentences from grams.

### [NER.ipynb](https://github.com/Jonashellevang/IE_MBD_2020/blob/master/Natural%20Language%20Processing/Class%20Exercises/NER.ipynb)
In this practical lab we focused on one of the main Information Extraction methodologies: Named Entity Recognition (NER). NERs focus is to to detect and classify the names in the text.

In the theoretical session we also presented three main methodologies to address the recognition of named entities:

* Hidden Markov Models
* MaxEnt Markov Models
* Conditional Random Fields

### [POS Tagging.ipynb](https://github.com/Jonashellevang/IE_MBD_2020/blob/master/Natural%20Language%20Processing/Class%20Exercises/POS%20Tagging.ipynb)
In this notebook we went through the process of POS tagging.

### [Semantics.ipynb](https://github.com/Jonashellevang/IE_MBD_2020/blob/master/Natural%20Language%20Processing/Class%20Exercises/Semantics.ipynb)
NLTK provides a useful WordNet interface to play with the WordNet data (included into the nltk.corpus). In this notebook we learned how to use it.

# Disaster Tweets (Kaggle)
On a general note, this assignment was in particular interesting because even though concepts and theory from class was understood, we still had not run any sort of machine learning process like this. It was not until after the process of EDA I understood that when we vectorize the text, creating new features and doing “normal” machine learning processes are completely different. For me, that was the biggest discovery of this assignment. The machine learning process is way different, and that highlights the importance of proper cleaning and processing of the text.

We used the Kaggle competiton [Real or Not? NLP with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started), and both the report and python notebook is incluced in this repository. You can access the notebook by clicking the file "Disaster Tweets.ipynb" or [this link](https://github.com/Jonashellevang/IE_MBD_2020/blob/master/Natural%20Language%20Processing/Disaster%20Tweets.ipynb) and the report can be accessed by clicking the file "Disaster Tweets Report.pdf" or [this link](https://github.com/Jonashellevang/IE_MBD_2020/blob/master/Natural%20Language%20Processing/Disaster%20Tweets%20Report.pdf).

# The Great Firewall of China (SOA Report)
Natural language processing techniques cover a wide variety of practical applications. NLP can be used for everything from judging sentiment to identifying spam, but one of the most dynamic applications is through encoding and decoding. While the idea of “codes” might bring to mind Indiana Jones movies, the modern use cases more often concern hiding content from government and private companies, and on the other hand, decoding said information. Tight censoring regulations in China have resulted in widespread use of modern language processing, with citizens developing an ever-evolving lexicon of codewords, while the government and social platforms try to keep up. This chase has led to a game of cat-and-mouse, that while deeply restrictive for the Chinese people, has also created practical applications for state-of-the-art natural language processing techniques.

The report can be accessed by clicking the file "The Great Firewall of China Report.pdf" or by clicking [this link](https://github.com/Jonashellevang/IE_MBD_2020/blob/master/Natural%20Language%20Processing/The%20Great%20Firewall%20of%20China%20Report.pdf).
